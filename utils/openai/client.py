import os
import asyncio
from typing import Optional, Dict, Any, List
import json

try:
    import openai
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class OpenAIClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API
    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Claude –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –≤ Telegram –∞–≥–µ–Ω—Ç–µ
    """
    
    def __init__(self):
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI library –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
        
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞
        self.client = OpenAI(api_key=self.api_key)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.default_model = "gpt-4"
        self.fallback_model = "gpt-3.5-turbo"
        
        print("ü§ñ OpenAI Client –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def generate_response(
        self,
        prompt: str,
        agent_id: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            agent_id: ID –∞–≥–µ–Ω—Ç–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è OpenAI)
            model: –ú–æ–¥–µ–ª—å OpenAI (gpt-4, gpt-3.5-turbo)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
            selected_model = model or self.default_model
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = await self._generate_with_openai_api(
                prompt, selected_model, max_tokens, temperature
            )
            
            return response
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ OpenAI: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ —Å —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é
            if model != self.fallback_model:
                try:
                    print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é {self.fallback_model}")
                    return await self._generate_with_openai_api(
                        prompt, self.fallback_model, max_tokens, temperature
                    )
                except Exception as fallback_error:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {fallback_error}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ–± –æ—à–∏–±–∫–µ
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
    
    async def _generate_with_openai_api(
        self,
        prompt: str,
        model: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π API OpenAI"""
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {e}")
            raise
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> str:
        """
        –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —á–∞—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Å–æ–æ–±—â–µ–Ω–∏–π
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}]
            model: –ú–æ–¥–µ–ª—å OpenAI
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        try:
            selected_model = model or self.default_model
            
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=selected_model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=messages
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ chat completion OpenAI: {e}")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é
            if model != self.fallback_model:
                try:
                    response = await asyncio.to_thread(
                        self.client.chat.completions.create,
                        model=self.fallback_model,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        messages=messages
                    )
                    return response.choices[0].message.content.strip()
                except:
                    pass
            
            raise
    
    def test_connection(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å OpenAI"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
            response = self.client.chat.completions.create(
                model=self.fallback_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∞
                max_tokens=5,
                messages=[
                    {
                        "role": "user",
                        "content": "Hi"
                    }
                ]
            )
            
            return len(response.choices) > 0 and response.choices[0].message.content
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è OpenAI: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return [
            "gpt-4",
            "gpt-4-turbo-preview",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k"
        ]
    
    async def create_agent_session(self, session_id: str, system_prompt: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è"""
        return {
            "session_id": session_id,
            "system_prompt": system_prompt,
            "messages": [],
            "created_at": asyncio.get_event_loop().time(),
            "provider": "openai"
        }
    
    async def add_message_to_session(
        self,
        session_id: str,
        role: str,
        content: str,
        sessions_storage: Dict[str, Dict]
    ):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–µ—Å—Å–∏—é"""
        if session_id in sessions_storage:
            sessions_storage[session_id]["messages"].append({
                "role": role,
                "content": content
            })
    
    async def generate_from_session(
        self,
        session_id: str,
        sessions_storage: Dict[str, Dict],
        model: Optional[str] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Å–µ—Å—Å–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        if session_id not in sessions_storage:
            raise ValueError(f"–°–µ—Å—Å–∏—è {session_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        session = sessions_storage[session_id]
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        messages = []
        
        if session["system_prompt"]:
            messages.append({
                "role": "system",
                "content": session["system_prompt"]
            })
        
        messages.extend(session["messages"])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = await self.chat_completion(messages, model)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–µ—Å—Å–∏—é
        await self.add_message_to_session(
            session_id, "assistant", response, sessions_storage
        )
        
        return response
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        model_info = {
            "gpt-4": {
                "name": "GPT-4",
                "max_tokens": 8192,
                "cost_per_1k": 0.03,
                "description": "–°–∞–º–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å OpenAI"
            },
            "gpt-4-turbo-preview": {
                "name": "GPT-4 Turbo Preview",
                "max_tokens": 128000,
                "cost_per_1k": 0.01,
                "description": "–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-4 —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
            },
            "gpt-3.5-turbo": {
                "name": "GPT-3.5 Turbo",
                "max_tokens": 4096,
                "cost_per_1k": 0.002,
                "description": "–ë—ã—Å—Ç—Ä–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å"
            },
            "gpt-3.5-turbo-16k": {
                "name": "GPT-3.5 Turbo 16K",
                "max_tokens": 16384,
                "cost_per_1k": 0.004,
                "description": "GPT-3.5 —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
            }
        }
        
        return model_info.get(model, {
            "name": model,
            "max_tokens": 4096,
            "cost_per_1k": 0.002,
            "description": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å"
        })
    
    async def estimate_tokens(self, text: str) -> int:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
        # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—à–µ
        return len(text) // 3  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    
    def format_telegram_context(
        self,
        system_instruction: str,
        context_messages: List[Dict],
        trigger_message: str,
        example_replies: Optional[str] = None
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è OpenAI –≤ —Å—Ç–∏–ª–µ Telegram –∞–≥–µ–Ω—Ç–∞
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –¥–ª—è Claude
        """
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        context_text = ""
        if context_messages:
            context_text = "\n".join([
                f"[{msg.get('date', 'Unknown')}] {msg.get('text', '')}" 
                for msg in context_messages
            ])
        
        # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤
        examples_text = ""
        if example_replies:
            examples_text = f"\n\n–ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤: {example_replies}"
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"""–°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {system_instruction}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π:
{context_text}

–°–æ–æ–±—â–µ–Ω–∏–µ-—Ç—Ä–∏–≥–≥–µ—Ä: {trigger_message}{examples_text}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–Ω—É –±–µ—Å–µ–¥—ã."""

        return prompt